{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Notebook 2 – Natural Language Understanding (NLU)\n",
    "NLU analyzes text to extract meta-data from content such as concepts, entities, keywords, categories, relations and semantic roles.\n",
    "https://www.ibm.com/watson/services/natural-language-understanding/ \n",
    "https://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1/  \n",
    "\n",
    "\n",
    "## Install dependencies\n",
    "\n",
    "Python’s standard library is very extensive, offering a wide range of facilities. It contains built-in modules like JSON a lightweight data interchange format. https://docs.python.org/2/library/index.html and https://docs.python.org/2/library/json.html\n",
    "\n",
    "IBM Watson Developer Cloud has a Python client library to quickly get started with the various Watson APIs services. https://pypi.python.org/pypi/watson-developer-cloud\n",
    "\n",
    "Using Python with IBM COS: Python support is provided through the Boto 3 library. The boto3 library provides complete access and can source credentials. The IBM COS endpoint must be specified when creating a service resource or low-level client as shown in documentation https://ibm-public-cos.github.io/crs-docs/python\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports.... Run this each time after restarting the Kernel\n",
    "#!pip install watson_developer_cloud\n",
    "import watson_developer_cloud as watson\n",
    "import json\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cloud Object Storage - Add Credentials & Bucket Name\n",
    "If you've not already set up COS - please see Step 1\n",
    "\n",
    "### Credentials\n",
    "Credentials are also created for you when you create project. From service dashboard page select `Service Credentials` from left navigation menu item, and copy/paste the credentials below:\n",
    "\n",
    "### Bucket name\n",
    "Buckets are created for you when you create project. From service dashboard page select `Buckets` from left navigation menu item, and get your bucket name and copy/paste bucket name below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Cloud Object Storage - populate your own information here from \"SERVICES\" on this page, or Console Dashboard on ibm.com/cloud\n",
    "\n",
    "# From service dashboard page select Service Credentials from left navigation menu item\n",
    "credentials_os = {\n",
    "  \"apikey\": \"\",\n",
    "  \"cos_hmac_keys\": {\n",
    "    \"access_key_id\": \"\",\n",
    "    \"secret_access_key\": \"\"\n",
    "  },\n",
    "  \"endpoints\": \"https://cos-service.bluemix.net/endpoints\",\n",
    "  \"iam_apikey_description\": \"Auto generated apikey during resource-key operation for Instance\",\n",
    "  \"iam_apikey_name\": \"\",\n",
    "  \"iam_role_crn\": \"\",\n",
    "  \"iam_serviceid_crn\": \"\",\n",
    "  \"resource_instance_id\": \"\"\n",
    "}\n",
    "\n",
    "# Buckets are created for you when you create project. From service dashboard page select Buckets from left navigation menu item, \n",
    "credentials_os['BUCKET'] = '<bucket_name>' # copy bucket name from COS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code was removed by DSX for sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Watson Natural Language Understanding (NLU) service\n",
    "\n",
    "Two options to create a new NLU service.  (1) Above click SERVICES and create/add new LITE version of NLU; or (2) In Console Dashboard in ibm.com/cloud create a LITE NLU services.  Click on 'SERVICE CREDENTIALS' to get creds.\n",
    "\n",
    "For more information on creating Watson services, see Notebook 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_nlu = {\n",
    "    \"url\": \"\",\n",
    "    \"apikey\": \"\",\n",
    "    \"version\": \"2017-02-27\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Object Storage Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoints = requests.get(credentials_os['endpoints']).json()\n",
    "\n",
    "iam_host = (endpoints['identity-endpoints']['iam-token'])\n",
    "cos_host = (endpoints['service-endpoints']['cross-region']['us']['public']['us-geo'])\n",
    "\n",
    "auth_endpoint = \"https://\" + iam_host + \"/oidc/token\"\n",
    "service_endpoint = \"https://\" + cos_host\n",
    "\n",
    "\n",
    "client = ibm_boto3.client(\n",
    "    's3',\n",
    "    ibm_api_key_id = credentials_os['apikey'],\n",
    "    ibm_service_instance_id = credentials_os['resource_instance_id'],\n",
    "    ibm_auth_endpoint = auth_endpoint,\n",
    "    config = Config(signature_version='oauth'),\n",
    "    endpoint_url = service_endpoint\n",
    "   )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLU\n",
    "\n",
    "- `process_text()` goes throught the text and fetch sentences and concatenate transcript based on chunk size\n",
    "- `analyze transcript()` calls natural language understanding endpoint and analyze the transcripe\n",
    "- `post_analysis` processes the results and show insights based on response from NLU endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLU\n",
    "\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "from watson_developer_cloud.natural_language_understanding.features import (\n",
    "    v1 as Features)\n",
    "\n",
    "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "    version = '2017-02-27',\n",
    "    iam_apikey = credentials_nlu['apikey'],\n",
    ")\n",
    "\n",
    "chunk_size=25 # This CHUNK size is used to disaggregate a transcript \n",
    "#e.g. in this case a 290 word transcript would have 10 chunks - 9 with 30 words and 1 with 20 words - approximates 'time domain' for this lab\n",
    "\n",
    "def chunk_transcript(transcript, chunk_size):\n",
    "    transcript = transcript.split(' ')\n",
    "    return [ transcript[i:i+chunk_size] for i in range(0, len(transcript), chunk_size) ] # chunking data\n",
    "\n",
    "def process_text(text):\n",
    "    transcript=''\n",
    "    for sentence in json.loads(text)['results']:\n",
    "        transcript = transcript + sentence['alternatives'][0]['transcript'] # concatenate sentences\n",
    "    #transcript = chunk_transcript(transcript, chunk_size) # chunk the transcript\n",
    "    return  transcript\n",
    "\n",
    "\n",
    "def analyze_transcript(features, file_name):\n",
    "    streaming_body = client.get_object(Bucket = credentials_os['BUCKET'], Key=file_name.split('.')[0]+'_text.json')['Body']\n",
    "    transcript = process_text(streaming_body.read().decode(\"utf-8\"))\n",
    "    nlu_analysis = natural_language_understanding.analyze(features, transcript, return_analyzed_text=True).get_result()\n",
    "    res=client.put_object(Bucket = credentials_os['BUCKET'], Key=file_name.split('.')[0]+'_NLU.json', Body= json.dumps(nlu_analysis))\n",
    "    return nlu_analysis\n",
    "\n",
    "def post_analysis(result):\n",
    "    print(result['analyzed_text'])\n",
    "    categories = result['categories']\n",
    "    for category in categories:\n",
    "        print('label: ', category['label'], ', score: ', category['score']) #add table instead of prints\n",
    "\n",
    "        \n",
    "def process_text_chunks(text):\n",
    "    transcript=''\n",
    "    for sentence in json.loads(text)['results']:\n",
    "        transcript = transcript + sentence['alternatives'][0]['transcript'] # concatenate sentences\n",
    "    transcript = chunk_transcript(transcript, chunk_size) # chunk the transcript\n",
    "    return  transcript\n",
    "\n",
    "def analyze_transcript_chunks(features, file_name):\n",
    "    streaming_body = client.get_object(Bucket = credentials_os['BUCKET'], Key=file_name.split('.')[0]+'_text.json')['Body']\n",
    "    transcript=streaming_body.read().decode(\"utf-8\")\n",
    "    nlu_analysis={}\n",
    "    for chunk in process_text_chunks(transcript):\n",
    "        chunk = ' '.join(chunk)\n",
    "        print('chunk: ', chunk)\n",
    "        nlu_analysis[chunk] = natural_language_understanding.analyze(features, chunk, return_analyzed_text=True, language='en').get_result()\n",
    "    outfilename = file_name.split('.')[0]+'_NLUchunks.json'\n",
    "    print(\"writing file: \", outfilename, \" to cloud object storage\" )\n",
    "    res=client.put_object(Bucket = credentials_os['BUCKET'], Key=outfilename, Body= json.dumps(nlu_analysis))\n",
    "    return nlu_analysis\n",
    "\n",
    "\n",
    "def post_analysis_chunks(result):\n",
    "    for chunk in result.keys():\n",
    "        categories = result[chunk]['categories']\n",
    "        print('\\nchunk: ', chunk)\n",
    "        for category in categories:\n",
    "            print('label: ', category['label'], ', score: ', category['score']) #add table instead of prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = ['sample1-addresschange-positive.ogg',\n",
    "             'sample2-address-negative.ogg',\n",
    "             'sample3-shirt-return-weather-chitchat.ogg',\n",
    "             'sample4-angryblender-sportschitchat-recovery.ogg',\n",
    "             'sample5-calibration-toneandcontext.ogg',\n",
    "             'jfk_1961_0525_speech_to_put_man_on_moon.ogg',\n",
    "             'May 1 1969 Fred Rogers testifies before the Senate Subcommittee on Communications.ogg']\n",
    "\n",
    "features = {\"concepts\":{},\"entities\":{},\"keywords\":{},\"categories\":{},\"emotion\":{},\"sentiment\":{},\"semantic_roles\":{} }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will run NLU enrichment on the transcripts for all audio files. We show two approaches:\n",
    "* One NLU call per audio file: In this case, we get aggregated features for the complete audio file.\n",
    "* One NLU call per chunk of audio file where a chunk is 25 words: In this case, we get more granular NLU features.\n",
    "\n",
    "Both approaches are valid. The default one we show is the second approach with chunks as that provides more granular sentiment results. In practice, you can decide which is more relevant to your application. If you'd like to try the first approach, you'll need to uncomment the next cell and comment out the cell after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = analyze_transcript_chunks(features, file_list[0])\n",
    "post_analysis_chunks(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you'd like to execute NLU per chunk of audio file (chunk is 25 words), make sure the next lines are uncomments\n",
    "for filename in file_list:\n",
    "    print(\"\\n\\nprocessing file: \", filename)\n",
    "    result = analyze_transcript_chunks(features,filename)\n",
    "    post_analysis_chunks(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
